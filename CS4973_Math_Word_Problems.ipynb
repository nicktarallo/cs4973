{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20728c58-831b-4ee5-aee0-d68ad9216c08",
   "metadata": {},
   "source": [
    "Kaito Minami, Nick Tarallo\n",
    "\n",
    "CS 4973 Math Word Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d5fc2-88f5-4f3b-8f71-585be5235909",
   "metadata": {},
   "source": [
    "# Math Word Problems\n",
    "In this project, we will use an LLM to solve math word problems, such as this one 1:\n",
    "\n",
    "Very early this morning, Elise left home in a cab headed for the hospital. Fortunately, the roads were clear, and the cab company only charged her a base price of $3, and $4 for every mile she traveled. If Elise paid a total of $23, how far is the hospital from her house?\n",
    "\n",
    "We will explore several prompting strategies, some of which will be more effective than the others at solving math word problems. However, all the strategies are quite generic and will be broadly applicable to a wide variety of tasks. We will also use this project to introduce the OpenAI Completions API, which is a widely used API for LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb1222-39f3-426c-b751-1a541bd35f86",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "You will need to comfortable with text processing and regular expressions. If you are not, we recommend the following resources:\n",
    "\n",
    "1. Chapter 2 of Speech and Language Processing by Dan Jurafsky and James H. Martin is an in-depth introduction to regular expressions.\n",
    "\n",
    "2. The Regular Expression HOWTO by A.M. Kuchling is a gentler introduction to regular expressions in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aec717-8686-4131-82ea-6444a1edd4e4",
   "metadata": {},
   "source": [
    "## The Completions API\n",
    "The LLM that we will use in this assignment is Meta Llama 3.1 (8B). (We are deliberately not using Llama 3.1 Instruct, which is the instruction-tuned or “chat model”. The instruction-tuned model is even more capable on math word problems. However, the techniques that we will explore are also useful when working with instruction-tuned model to solve harder problems than math word problems.) Although it is a relatively small LLM, it is very capable for its size. The following code shows you how to query it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f891044-e165-4159-8493-e51791d8c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=URL, api_key=KEY)\n",
    "\n",
    "resp = client.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=100,\n",
    "    stop=[\"\\n\"],\n",
    "    prompt=\"Shakespeare was\"\n",
    "\n",
    ")\n",
    "print(resp.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb3073-155e-4363-a9ea-a46b45a0d384",
   "metadata": {},
   "source": [
    "When you run this code, you may see something like this:\n",
    "\n",
    "born in Stratford-upon-Avon, England, in 1564. His father was a glove-maker and his mother was the daughter of a landowner. Shakespeare attended grammar school in Stratford, and his education was broad and rigorous. He was well versed in literature, history, and the classics. At the age of 18, he married Anne Hathaway, who was eight years his senior. The couple had three children: Susanna, Hamnet, and Judith. Shakespeare began\n",
    "\n",
    "You should play around with the code above and review the Completions API Reference and the Completions Guide. Note that the model that we host does not support every optional argument that the API documents. However, you can set the stop sequences, set the sampling temperature, setup nucleus sampling, and control the generation length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3931e17-d74c-48e4-9b38-c880bda0bc84",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting\n",
    "A base model is not specifically designed to answer questions. All it does is complete the prompt with likely text. For example, if we prompt the model with exactly the text of the math word problem above, you may get an answer, an explanation, or even a continuation of the problem. For example, after five attempts at temperature 0.2, I got the model to produce the following hint instead of the answer:\n",
    "\n",
    "Hint: You can use the equation 3 + 4x = 23, where x is the distance in miles.\n",
    "\n",
    "**Task 1:** Your first task is to figure out how to prompt the model so that it fairly reliably produces an answer for a math word problem: an answer that is always a number. To do so, write a pair of functions that (1) take a math word problem and turns it into a prompt that elicits a direct answer from the LLM and (2) takes the LLM response, which will always be a string, and turns it into a number. The latter function should return None if the LLM does not produce a number as directed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdac36dd-f5aa-4a1c-a827-3d3ec596d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "def prompt_zero_shot(problem: str) -> str:\n",
    "    # Your code here\n",
    "\n",
    "def extract_zero_shot(completion: str) -> Optional[int]:\n",
    "    # Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60032af1-534e-4684-91c6-71e60c16dab2",
   "metadata": {},
   "source": [
    "The two functions above should not use the Completions API. Instead, put them together using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70454f0a-9c87-4b79-89df-6e39548706d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_zero_shot(problem: str) -> Optional[int]\n",
    "    resp = client.completions.create(\n",
    "        model=\"meta-llama/Llama-3.1-8B\",\n",
    "        temperature=0.2,\n",
    "        messages=prompt_zero_shot(problem)\n",
    "    )\n",
    "    return extract_zero_shot(resp.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9894562-dac4-4646-97b2-ab108bfa0c65",
   "metadata": {},
   "source": [
    "**Task 2:** For this task, you will work with a list of math word problems and their answers. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a3e1f-26bb-47fc-8708-1828c4687fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "EASY = [\n",
    "    { \n",
    "        \"problem\": \"I ate 2 pears and 1 apple. How many fruit did I eat?\",\n",
    "        \"answer\": 3 \n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"I had ten chocolates but ate one. How many remain?\",\n",
    "        \"answer\": 9\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca99c2-877d-48c6-87d3-b9703393fe26",
   "metadata": {},
   "source": [
    "Your function should take a list of problems, such as the one above, and compute the accuracy of the LLM on that list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68964f9d-f1fd-4eed-a5c0-355d1dfd55e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_zero_shot(problems: List[dict]) -> float:\n",
    "    # Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeecbef2-1ddd-49b1-adb4-8090622d3c70",
   "metadata": {},
   "source": [
    "Your code must not raise an exception, no matter what the LLM returns. So, make sure you handle any exceptions raised by solve_zero_shot.\n",
    "\n",
    "**Task 3:** The dataset nuprl/llm-systems-math-word-problems has 50 math word problems in its test set, and you can load it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc49818-4161-4311-84c4-43f8a71181df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "TEST = datasets.load_dataset(\"nuprl/llm-systems-math-word-problems\", split=\"test\")\n",
    "\n",
    "print(accuracy_zero_shot(TEST))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf4e9e-def9-400b-8f10-d45a2a89d3da",
   "metadata": {},
   "source": [
    "What accuracy do you get? Try re-running accuracy_zero_shot a few times. If you’re sampling with temperature, you will see that the result can vary significantly on each run. To get a stable result, update accuracy_zero_shot to try each problem n=5 times and report mean accuracy for each attempt.\n",
    "\n",
    "For full credit, you need to get at least 10% accuracy and a stable result. If not, you can try to improve accuracy in a few ways:\n",
    "\n",
    "1. You can try to improve the prompt in prompt_zero_shot. But, your prompt must still elicit a direct answer and should not given examples. (In the next part, we will use more sophisticated prompting techniques to elicit more complex responses.)\n",
    "\n",
    "2. You may find that extract_zero_shot fails when the LLM produces strings such as \"$23\" or \"1,200\" Feel free to address these.\n",
    "\n",
    "3. Experiment with different generation hyperparameters, such as temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46041a78-3f03-49f7-993f-eb7044a0d996",
   "metadata": {},
   "source": [
    "### Tracking Progress\n",
    "When you have a long list of problems, you will find it helpful to track progress. You could print after each problem, but that will fill up your screen quickly. Alternatively, use the tqdm library to display a compact progress bar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c87b0-7d98-49fe-bd11-22a7e0b8ee50",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting\n",
    "With a zero-shot prompt, we are giving the model very limited guidance on what kind of answer we want. In fact, your zero-shot prompt was unlikely to be 100% reliable. There were probably a few problems where it did not produce a number. To address this, we’ll now explore few-shot prompting.2\n",
    "\n",
    "**Task 4:** Implement the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43e8d7-dbc7-49cf-a041-16a477551ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_few_shot(problem: str) -> str:\n",
    "    # Your code here\n",
    "\n",
    "def extract_few_shot(completion: str) -> Optional[int]:\n",
    "    # Your code here\n",
    "\n",
    "def solve_few_shot(problem: str) -> Optional[int]:\n",
    "    # Your code here\n",
    "\n",
    "def accuracy_few_shot(problems: List[dict]) -> float:    \n",
    "    # Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a33ab-fd49-4773-a007-e8d0e9f64c7e",
   "metadata": {},
   "source": [
    "The prompt that you construct should have a few example problems and answers. With your few-shot prompt, you should always get a numeric answer. However, we do not expect the accuracy to increase by very much over the zero-shot version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed334c-e834-44ff-823f-c936b5e7423e",
   "metadata": {},
   "source": [
    "## Chain-of-Thought Prompting\n",
    "**For the rest of this assignment, feel free to use Copilot or other kinds of GenerativeAI.**\n",
    "\n",
    "We’ll now explore chain-of-thought (COT) prompting, and see that it significantly increases accuracy on the task.3 For example, consider the following problem:\n",
    "\n",
    "Henry and 3 of his friends order 7 pizzas for lunch. Each pizza is cut into 8 slices. If Henry and his friends want to share the pizzas equally, how many slices can each of them have?\n",
    "\n",
    "Here is one way to reason through the answer:\n",
    "\n",
    "7 pizzas are cut into 8 slices each. Thus the total number of slices is 7 * 8 = 56. Henry and 3 friends want to share the pizza equally, so the slices are divided between 4 people. Each person gets 56 / 4 = 14 slices.\n",
    "\n",
    "**Task 5:** Your task is the implement the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301650dd-4c67-49a1-b487-86e55aff07de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_cot(problem: str) -> str:\n",
    "    # Your code here\n",
    "\n",
    "def extract_cot(completion: str) -> Optional[int]:\n",
    "    # Your code here\n",
    "\n",
    "def solve_cot(problem: str) -> Optional[int]:\n",
    "    # Your code here\n",
    "\n",
    "def accuracy_cot(problems: List[dict]) -> float:    \n",
    "    # Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c48679-ddf0-49e7-88f7-cd20c5909690",
   "metadata": {},
   "source": [
    "Start by writing the prompting function, which should prefix the problem with 2-3 COT examples. You’ll need to write the “thoughts” yourself. Do not use the problems from test split for the COT examples. Instead, you may use the problems from train split, or construct your own.\n",
    "\n",
    "Given that your prompt elicits “thoughts” from the model, you will need to carefully extract the final answer in extract_cot. This will require more work than what you did in the earlier approaches. For full credit, you need to get at least 45% accuracy and a stable result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989b390-7e75-4ce1-b560-e8c4f1a057f0",
   "metadata": {},
   "source": [
    "## Program-Aided Language Models\n",
    "For the final part, we will explore program-aided language models (PAL).4 PAL is a variation of chain-of-thought, but instead of prompting the model to reason in natural language, we prompt the model to produce a program that solves the problem, and then run that program.\n",
    "\n",
    "For example, consider the example problem in the COT part. Instead of producing natural language, we could instead produce the following program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0bbab7-6535-4aed-9354-9ca3797ba7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pizzas = 7\n",
    "slices_per_pizza = 8\n",
    "total_slices = num_pizzas * slices_per_pizza\n",
    "num_people = 1 + 3\n",
    "slices_per_person = total_slices / num_people\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382f095-84d2-44d9-8a40-1b2165aa3ba7",
   "metadata": {},
   "source": [
    "When we run this program, the value of slices_per_person will be the answer.\n",
    "\n",
    "Dynamically running code: We strongly recommend encoding each program as a function that returns the answer. If you have a string that represents a function, you can define it with the builtin exec and get its result with the builtin eval. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e5f3b-5969-42b4-ac02-e9e789becab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE = \"\"\"\n",
    "def my_func():\n",
    "    return 1 + 1\n",
    "\"\"\".strip()\n",
    "\n",
    "exec(CODE)\n",
    "x = eval(\"my_func()\")\n",
    "assert x == 2\n",
    "\n",
    "# If you know the name of the function, you can \n",
    "# call it without eval.\n",
    "y = my_func()\n",
    "assert y == 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a85370-a80d-465e-a056-b4c198f2575d",
   "metadata": {},
   "source": [
    "**Task 6:** Your task is to implement PAL with the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736cfef4-e500-4f89-9569-6646a57fda78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_pal(problem: str) -> str:\n",
    "    # Your code here\n",
    "\n",
    "def extract_pal(completion: str) -> Optional[int]:\n",
    "    # Your code here. Use exec and eval.\n",
    "\n",
    "def solve_pal(problem: str) -> Optional[int]:\n",
    "    # Your code here\n",
    "\n",
    "def accuracy_pal(problems: List[dict]) -> float:    \n",
    "    # Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c032891-1f73-45c7-a574-0d695082f304",
   "metadata": {},
   "source": [
    "For full credit, you need to get at least 60% accuracy and a stable result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b73033-6569-4cd1-a95f-72fdeaff4c87",
   "metadata": {},
   "source": [
    "## What to Submit\n",
    "You should submit two files. First, a Python file that implements all the functions above. Running this file should have no side-effects. Second, a Jupyter notebook that shows how you tested the work. This notebook should load the datasets and use the functions implemented in the first file. Make sure you save the cells’ outputs.\n",
    "\n",
    "1. The word problems in this assignment are from the GSM8K (grade school mathematics) benchmark (Cobbe et al., 2021). ↩\n",
    "\n",
    "2. The effectiveness of few-shot prompting was a key capability that distinguished GPT3 from GPT2 (Brown et al., 2020). ↩\n",
    "\n",
    "3. Wei et al., 2022 introduced chain of thought prompting. ↩\n",
    "\n",
    "4. Gao et al., 2023introduced program-aided language models. ↩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72056b8a-87ba-48de-a366-526739cd766a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
