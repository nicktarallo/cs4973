{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import datasets\n",
    "import textwrap\n",
    "from collections import namedtuple\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have set the following environment variables in `~/.zshrc` (macOS) or `~/.bashrc` (Linux / WSL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = os.getenv(\"CS4973_BASE_URL\")\n",
    "API_KEY=api_key=os.getenv(\"CS4973_API_KEY\")\n",
    "# BASE_URL = \"http://199.94.61.113:8000/v1/\"\n",
    "# API_KEY = \"minami.k@northeastern.edu:Zrw5ZdEopph2HKOWIv6w\"\n",
    "assert BASE_URL is not None\n",
    "assert API_KEY is not None\n",
    "\n",
    "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "def llama3(prompt, **kwargs):\n",
    "    response = client.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B\",\n",
    "        prompt=prompt,\n",
    "        **kwargs)\n",
    "    return response.choices[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get used to the [Hugging Face\n",
    "Datasets](https://huggingface.co/docs/datasets/) library. It is widely used for\n",
    "public benchmark problems, and for proprietary datasets. We are going to use it\n",
    "load some problems from BIG-Bench Hard ([Suzgun et al,\n",
    "2019](https://arxiv.org/abs/2210.09261)).\n",
    "\n",
    "The code below loads the `\"maveriq/bigbenchhard\"` dataset and select the\n",
    "`\"reasoning_about_colored_objects\"` configuration and the `\"train\" split within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset big_bench_hard (/Users/Kaito.01/.cache/huggingface/datasets/maveriq___big_bench_hard/reasoning_about_colored_objects/0.0.0/8168fc1d2bddccf47b239e3a0b879d3eded200ae8a6bbdc5afc2a143acf88010)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbh = datasets.load_dataset(\"maveriq/bigbenchhard\", \"reasoning_about_colored_objects\", split=\"train\")\n",
    "bbh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the floor, there is one mauve cat toy, two purple cat toys, three grey cat toys, two mauve notebooks, three grey notebooks, three burgundy cat toys, and one purple notebook. If I remove all the notebooks from the floor, how many grey objects remain on it?\n",
      "Options:\n",
      "(A) zero\n",
      "(B) one\n",
      "(C) two\n",
      "(D) three\n",
      "(E) four\n",
      "(F) five\n",
      "(G) six\n",
      "(H) seven\n",
      "(I) eight\n",
      "(J) nine\n",
      "(K) ten\n",
      "(L) eleven\n",
      "(M) twelve\n",
      "(N) thirteen\n",
      "(O) fourteen\n",
      "(P) fifteen\n",
      "(Q) sixteen\n"
     ]
    }
   ],
   "source": [
    "print(bbh[0][\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function makes each item a little easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the desk, you see several things arranged in a row: a burgundy bracelet, a\n",
      "grey mug, a green necklace, and a magenta textbook. What is the color of the\n",
      "thing directly to the left of the necklace?\n",
      "\n",
      "Answer: (P) grey\n"
     ]
    }
   ],
   "source": [
    "def inspect_bbh(item):\n",
    "    txt, options = item[\"input\"].split(\"Options:\", maxsplit=1)\n",
    "    txt = textwrap.fill(txt, width=80)\n",
    "    for opt in options.split(\"\\n\"):\n",
    "        if item[\"target\"] in opt:\n",
    "            txt += f\"\\n\\nAnswer: {opt}\"\n",
    "            break\n",
    "    return txt\n",
    "\n",
    "print(inspect_bbh(bbh[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BBH problems are quite hard. Llama3.1-8B-Base doesn't do very when using\n",
    "zero-shot prompting. We'll try once, and also try to write an evaluation loop\n",
    "that saves the failures in `zero_shot_failures`. Approach is to write these\n",
    "functions: `prompt_zero_shot`, `extract_zero_shot`, `solve_zero_shot`, and\n",
    "`accuracy_zero_shot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'On the floor, there is one mauve cat toy, two purple cat toys, three grey cat toys, two mauve notebooks, three grey notebooks, three burgundy cat toys, and one purple notebook. If I remove all the notebooks from the floor, how many grey objects remain on it?\\nOptions:\\n(A) zero\\n(B) one\\n(C) two\\n(D) three\\n(E) four\\n(F) five\\n(G) six\\n(H) seven\\n(I) eight\\n(J) nine\\n(K) ten\\n(L) eleven\\n(M) twelve\\n(N) thirteen\\n(O) fourteen\\n(P) fifteen\\n(Q) sixteen',\n",
       " 'target': '(D)'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Meta-Llama-3.1-8B` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_zero_shot\u001b[39m(response):\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m---> 10\u001b[0m llama3(prompt_zero_shot(bbh[\u001b[39m40\u001b[39;49m]), temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, max_tokens\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36mllama3\u001b[0;34m(prompt, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama3\u001b[39m(prompt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 11\u001b[0m     response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     12\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmeta-llama/Meta-Llama-3.1-8B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     14\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ds/lib/python3.9/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ds/lib/python3.9/site-packages/openai/resources/completions.py:539\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    512\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    538\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Completion \u001b[39m|\u001b[39m Stream[Completion]:\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    540\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    541\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    542\u001b[0m             {\n\u001b[1;32m    543\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    544\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt,\n\u001b[1;32m    545\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mbest_of\u001b[39;49m\u001b[39m\"\u001b[39;49m: best_of,\n\u001b[1;32m    546\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mecho\u001b[39;49m\u001b[39m\"\u001b[39;49m: echo,\n\u001b[1;32m    547\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    548\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    549\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    550\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    551\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    552\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    553\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    554\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    555\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    556\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    557\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39msuffix\u001b[39;49m\u001b[39m\"\u001b[39;49m: suffix,\n\u001b[1;32m    558\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    559\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    560\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    561\u001b[0m             },\n\u001b[1;32m    562\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    563\u001b[0m         ),\n\u001b[1;32m    564\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    565\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    566\u001b[0m         ),\n\u001b[1;32m    567\u001b[0m         cast_to\u001b[39m=\u001b[39;49mCompletion,\n\u001b[1;32m    568\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    569\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[Completion],\n\u001b[1;32m    570\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ds/lib/python3.9/site-packages/openai/_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1247\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1248\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1255\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1256\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1257\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1258\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1259\u001b[0m     )\n\u001b[0;32m-> 1260\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ds/lib/python3.9/site-packages/openai/_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    936\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 937\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    938\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    939\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    940\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    941\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    942\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    943\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ds/lib/python3.9/site-packages/openai/_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1040\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1044\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1045\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     retries_taken\u001b[39m=\u001b[39moptions\u001b[39m.\u001b[39mget_max_retries(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries) \u001b[39m-\u001b[39m retries,\n\u001b[1;32m   1050\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Meta-Llama-3.1-8B` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}"
     ]
    }
   ],
   "source": [
    "zero_shot_failures = [ ]\n",
    "\n",
    "def prompt_zero_shot(item):\n",
    "    return item[\"input\"] + \"\\n\\nCorrect option:\"\n",
    "\n",
    "def extract_zero_shot(response):\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "llama3(prompt_zero_shot(bbh[40]), temperature=0, max_tokens=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def solve_zero_shot(item):\n",
    "    response = extract_zero_shot(llama3(prompt_zero_shot(item), temperature=0, max_tokens=3))\n",
    "    if item[\"target\"] == response:\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Expected {item['target']} got {response}\")\n",
    "        return False\n",
    "\n",
    "solve_zero_shot(bbh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def accuracy_zero_shot(items):\n",
    "    num_correct = 0\n",
    "    failures = [ ]\n",
    "    for item in tqdm(items):\n",
    "        result = solve_zero_shot(item)\n",
    "        if result:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            failures.append(item)\n",
    "\n",
    "    return (num_correct / len(items), failures)\n",
    "\n",
    "\n",
    "accuracy, failures = accuracy_zero_shot(bbh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([ item[\"target\"] for item in failures ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few of these wrong answers and think through what the right answers should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try few-shot prompting. I haven't before class. I don't think it will be\n",
    "very effective. We'll write the same four functions as above. (Use GenAI.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "FEW_SHOT_PROMPT = bbh[0][\"input\"] + \"\\n\\nCorrect option: \" + bbh[0][\"target\"] + \"\\n\\n\" +  bbh[1][\"input\"] + \"\\n\\nCorrect option: \" + bbh[1][\"target\"] + \"\\n\\n\" +  bbh[2][\"input\"] + \"\\n\\nCorrect option: \" + bbh[2][\"target\"] + \"\\n\\n\"\n",
    "\n",
    "def prompt_fewshot_shot(item):\n",
    "    return FEW_SHOT_PROMPT + \"\\n\\n\" + item[\"input\"] + \"\\n\\nCorrect option:\"\n",
    "\n",
    "def extract_fewshot_shot(response):\n",
    "    return response.strip()\n",
    "\n",
    "def solve_fewshot_shot(item):\n",
    "    response = extract_fewshot_shot(llama3(prompt_fewshot_shot(item), temperature=0, max_tokens=3))\n",
    "    if item[\"target\"] == response:\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Expected {item['target']} got {response}\")\n",
    "        return False\n",
    "\n",
    "solve_fewshot_shot(bbh[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def accuracy_fewshot_shot(items):\n",
    "    num_correct = 0\n",
    "    failures = [ ]\n",
    "    for item in tqdm(items):\n",
    "        result = solve_fewshot_shot(item)\n",
    "        if result:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            failures.append(item)\n",
    "\n",
    "    return (num_correct / len(items), failures)\n",
    "\n",
    "\n",
    "accuracy, failures = accuracy_fewshot_shot(bbh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In chain-of-thought prompting, we construct a few-shot prompt, where the few-shot\n",
    "examples include an example of how one might reason through the problem. We do\n",
    "so below, using the reasoning steps from above. Notice how we format the prompt\n",
    "to include both the reasoning steps and an answer that we can extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "COT_PROMPT = \"\"\"\n",
    "Input: On the floor, there is one mauve cat toy, two purple cat toys, three grey cat toys, two mauve notebooks, three grey notebooks, three burgundy cat toys, and one purple notebook. If I remove all the notebooks from the floor, how many grey objects remain on it?\n",
    "Options: \n",
    "(A) zero\n",
    "(B) one\n",
    "(C) two\n",
    "(D) three\n",
    "(E) four\n",
    "(F) five\n",
    "(G) six\n",
    "(H) seven\n",
    "(I) eight\n",
    "(J) nine\n",
    "(K) ten\n",
    "(L) eleven\n",
    "(M) twelve\n",
    "(N) thirteen\n",
    "(O) fourteen\n",
    "(P) fifteen\n",
    "(Q) sixteen\n",
    "\n",
    "Reasoning: There are three grey notebooks and three grey cat toys, which is six grey objects. There are two mauve notebooks and three grey notebooks. If I remove all the notebooks from the floor, I remove three grey objects, which gives me three grey objects that remain.\n",
    "\n",
    "Answer: (D) three\n",
    "\n",
    "Done\n",
    "\n",
    "Input: On the desk, you see a set of things arranged in a row: a grey cup, a purple mug, and a blue teddy bear. What is the color of the thing directly to the right of the cup?\n",
    "Options:\n",
    "(A) red\n",
    "(B) orange\n",
    "(C) yellow\n",
    "(D) green\n",
    "(E) blue\n",
    "(F) brown\n",
    "(G) magenta\n",
    "(H) fuchsia\n",
    "(I) mauve\n",
    "(J) teal\n",
    "(K) turquoise\n",
    "(L) burgundy\n",
    "(M) silver\n",
    "(N) gold\n",
    "(O) black\n",
    "(P) grey\n",
    "(Q) purple\n",
    "(R) pink\n",
    "\n",
    "Reasoning: The purple mug is directly to the right of the cup. The list is arranged from left to right.\n",
    "\n",
    "Answer: (Q) purple\n",
    "\n",
    "Done\n",
    "\n",
    "Input: On the nightstand, you see a set of items arranged in a row: a gold plate, a silver stress ball, a fuchsia notebook, a mauve bracelet, a green jug, and a yellow fidget spinner. What is the color of the item directly to the left of the jug?\n",
    "Options:\n",
    "(A) red\n",
    "(B) orange\n",
    "(C) yellow\n",
    "(D) green\n",
    "(E) blue\n",
    "(F) brown\n",
    "(G) magenta\n",
    "(H) fuchsia\n",
    "(I) mauve\n",
    "(J) teal\n",
    "(K) turquoise\n",
    "(L) burgundy\n",
    "(M) silver\n",
    "(N) gold\n",
    "(O) black\n",
    "(P) grey\n",
    "(Q) purple\n",
    "(R) pink\n",
    "\n",
    "Reasoning: The list is arranged from left to right. The mauve bracelet appears immediately before the green jug.\n",
    "\n",
    "Answer: (I) mauve\n",
    "\n",
    "Done\"\"\".strip()\n",
    "\n",
    "\n",
    "def prompt_cot(item):\n",
    "    return COT_PROMPT + \"\\n\\nInput: \" + item[\"input\"] + \"\\n\\Reasoning: \"\n",
    "\n",
    "def extract_cot(response: str):\n",
    "    items = response.split(\"Answer: \")\n",
    "    if len(items) < 2:\n",
    "        return \"(Z)\"\n",
    "    return items[1].split(\" \")[0]\n",
    "\n",
    "print(llama3(prompt_cot(bbh[3]), temperature=0, max_tokens=200, stop=[\"Done\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def solve_cot(item):\n",
    "    raw_response = llama3(prompt_cot(item), temperature=0, max_tokens=150, stop=[\"Done\"])\n",
    "    response = extract_cot(raw_response)\n",
    "    if item[\"target\"] == response:\n",
    "        return True, raw_response\n",
    "    else:\n",
    "        print(f\"Expected {item['target']} got {response}\")\n",
    "        return False, raw_response\n",
    "    \n",
    "\n",
    "def accuracy_cot(items):\n",
    "    num_correct = 0\n",
    "    failures = [ ]\n",
    "    for item in tqdm(items):\n",
    "        result, thought = solve_cot(item)\n",
    "        if result:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            failures.append({ \"thought\": thought, **item })\n",
    "\n",
    "    return (num_correct / len(items), failures)\n",
    "\n",
    "\n",
    "accuracy, failures = accuracy_cot(bbh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(failures[0][\"input\"])\n",
    "print(\"***\")\n",
    "print(failures[0][\"thought\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "cot_failures = [ ]\n",
    "cot_successes = [ ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "01bdc39619e4a2073758666eb8156b7444f5ec1d54793bf710aafde55eb14215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
